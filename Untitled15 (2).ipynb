{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeF2Q_l8FmaH",
        "outputId": "5f1bbc55-d0ec-40aa-84ea-b0071710fca1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "checkpoint = \"Salesforce/codegen-2B-multi\"\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, truncation=True, max_length=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzSIKPO_J2a-"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJWWsj5CoCVm",
        "outputId": "db4693f1-9343-40d3-ca0b-a06c70c4cb0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: content/data/\n",
            "  inflating: content/data/trmm-krnl.cpp  \n",
            "   creating: content/data/outputs/\n",
            "   creating: content/data/outputs/fewshots/\n",
            "   creating: content/data/outputs/.ipynb_checkpoints/\n",
            "  inflating: content/data/madd-krnl.cpp  \n",
            "  inflating: content/data/processed-vadd-krnl.cpp  \n",
            "  inflating: content/data/processed-syrk-krnl.cpp  \n",
            "  inflating: content/data/ewmm-krnl.cpp  \n",
            "  inflating: content/data/processed-madd-krnl.cpp  \n",
            "  inflating: content/data/processed-jacobi_1d-krnl.cpp  \n",
            "  inflating: content/data/jacobi_1d-krnl.cpp  \n",
            "   creating: content/data/.ipynb_checkpoints/\n",
            "  inflating: content/data/syrk-krnl.cpp  \n",
            "  inflating: content/data/processed-ewmm-krnl.cpp  \n",
            "  inflating: content/data/processed-dotprod-krnl.cpp  \n",
            "  inflating: content/data/trmm-opt-krnl.cpp  \n",
            "  inflating: content/data/vadd-krnl.cpp  \n",
            "  inflating: content/data/processed-trmm-opt-krnl.cpp  \n",
            "  inflating: content/data/mm-krnl.cpp  \n",
            "   creating: content/data/fewshots_examples/\n",
            "  inflating: content/data/fewshots_examples/gesummv-medium_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/bicg-medium_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/heat-3d_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/gemver_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/gemm-ncubed_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/fdtd-2d-large_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/correlation_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/fdtd-2d_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/doitgen-red_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/doitgen_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/aes_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/adi_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/gemver-medium_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/3mm_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/gesummv_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/bicg-large_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/atax_kernel.c  \n",
            "  inflating: content/data/fewshots_examples/atax-medium_kernel.c  \n",
            "  inflating: content/data/processed-trmm-krnl.cpp  \n",
            "  inflating: content/data/dotprod-krnl.cpp  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt5guFFEIZqD",
        "outputId": "31e444b4-73dc-42f4-e1f9-3e507334b2e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgOLi9_QtfFP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mxX3hn9824D"
      },
      "outputs": [],
      "source": [
        "!python ../process_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki74go4PPIiR"
      },
      "source": [
        "# Few shot prompting with persona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qQvVNjJ9M1L"
      },
      "outputs": [],
      "source": [
        "fewshots_header = \"\"\"\n",
        "Consider the following input output pairs where input is a program and output is the program with High Level Synthesis pragmas inserted\\n\n",
        "\n",
        "Input:\n",
        "\n",
        "void kernel_fdtd_2d(int tmax,int nx,int ny,double ex[60][80],double ey[60][80],double hz[60][80],double _fict_[40])\n",
        "{\n",
        "  int t;\n",
        "  int i;\n",
        "  int j;\n",
        "  for (t = 0; t < 40; t++) {\n",
        "    for (j = 0; j < 80; j++) {\n",
        "      ey[0][j] = _fict_[t];\n",
        "    }\n",
        "    for (i = 1; i < 60; i++) {\n",
        "      for (j = 0; j < 80; j++) {\n",
        "        ey[i][j] = ey[i][j] - 0.5 * (hz[i][j] - hz[i - 1][j]);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for (i = 0; i < 60; i++) {\n",
        "\n",
        "      for (j = 1; j < 80; j++) {\n",
        "        ex[i][j] = ex[i][j] - 0.5 * (hz[i][j] - hz[i][j - 1]);\n",
        "      }\n",
        "    }\n",
        "    for (i = 0; i < 59; i++) {\n",
        "      for (j = 0; j < 79; j++) {\n",
        "        hz[i][j] = hz[i][j] - 0.7 * (ex[i][j + 1] - ex[i][j] + ey[i + 1][j] - ey[i][j]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "Output:\n",
        "#pragma ACCEL kernel\n",
        "\n",
        "void kernel_fdtd_2d(int tmax,int nx,int ny,double ex[60][80],double ey[60][80],double hz[60][80],double _fict_[40])\n",
        "{\n",
        "  int t;\n",
        "  int i;\n",
        "  int j;\n",
        "//#pragma scop\n",
        "\n",
        "#pragma ACCEL PIPELINE auto{__PIPE__L0}\n",
        "\n",
        "#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n",
        "  for (t = 0; t < 40; t++) {\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_0}\n",
        "    for (j = 0; j < 80; j++) {\n",
        "      ey[0][j] = _fict_[t];\n",
        "    }\n",
        "\n",
        "#pragma ACCEL PIPELINE auto{__PIPE__L0_1}\n",
        "\n",
        "#pragma ACCEL TILE FACTOR=auto{__TILE__L0_1}\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_1}\n",
        "    for (i = 1; i < 60; i++) {\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_1_0}\n",
        "      for (j = 0; j < 80; j++) {\n",
        "        ey[i][j] = ey[i][j] - 0.5 * (hz[i][j] - hz[i - 1][j]);\n",
        "      }\n",
        "    }\n",
        "\n",
        "#pragma ACCEL PIPELINE auto{__PIPE__L0_2}\n",
        "\n",
        "#pragma ACCEL TILE FACTOR=auto{__TILE__L0_2}\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_2}\n",
        "    for (i = 0; i < 60; i++) {\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_2_0}\n",
        "      for (j = 1; j < 80; j++) {\n",
        "        ex[i][j] = ex[i][j] - 0.5 * (hz[i][j] - hz[i][j - 1]);\n",
        "      }\n",
        "    }\n",
        "\n",
        "#pragma ACCEL PIPELINE auto{__PIPE__L0_3}\n",
        "\n",
        "#pragma ACCEL TILE FACTOR=auto{__TILE__L0_3}\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_3}\n",
        "    for (i = 0; i < 59; i++) {\n",
        "\n",
        "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_3_0}\n",
        "      for (j = 0; j < 79; j++) {\n",
        "        hz[i][j] = hz[i][j] - 0.7 * (ex[i][j + 1] - ex[i][j] + ey[i + 1][j] - ey[i][j]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "//#pragma endscop\n",
        "}\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2oMPyDB_BX9"
      },
      "outputs": [],
      "source": [
        "prompt = fewshots_header + \"\"\"\n",
        "Act as an expert in High Level Synthesis, insert High Level Synthesis pragma to the folloing program. Reason your choice of High Level Synthesis pragma in comment.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaNdTtfe_k1H",
        "outputId": "91600d84-da1e-4481-fe66-c6740822770c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Consider the following input output pairs where input is a program and output is the program with High Level Synthesis pragmas inserted\n",
            "\n",
            "\n",
            "Input:\n",
            "\n",
            "void kernel_fdtd_2d(int tmax,int nx,int ny,double ex[60][80],double ey[60][80],double hz[60][80],double _fict_[40])\n",
            "{\n",
            "  int t;\n",
            "  int i;\n",
            "  int j;\n",
            "  for (t = 0; t < 40; t++) {\n",
            "    for (j = 0; j < 80; j++) {\n",
            "      ey[0][j] = _fict_[t];\n",
            "    }\n",
            "    for (i = 1; i < 60; i++) {\n",
            "      for (j = 0; j < 80; j++) {\n",
            "        ey[i][j] = ey[i][j] - 0.5 * (hz[i][j] - hz[i - 1][j]);\n",
            "      }\n",
            "    }\n",
            "    \n",
            "    for (i = 0; i < 60; i++) {\n",
            "      \n",
            "      for (j = 1; j < 80; j++) {\n",
            "        ex[i][j] = ex[i][j] - 0.5 * (hz[i][j] - hz[i][j - 1]);\n",
            "      }\n",
            "    }\n",
            "    for (i = 0; i < 59; i++) {\n",
            "      for (j = 0; j < 79; j++) {\n",
            "        hz[i][j] = hz[i][j] - 0.7 * (ex[i][j + 1] - ex[i][j] + ey[i + 1][j] - ey[i][j]);\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "Output:\n",
            "#pragma ACCEL kernel\n",
            "\n",
            "void kernel_fdtd_2d(int tmax,int nx,int ny,double ex[60][80],double ey[60][80],double hz[60][80],double _fict_[40])\n",
            "{\n",
            "  int t;\n",
            "  int i;\n",
            "  int j;\n",
            "//#pragma scop\n",
            "  \n",
            "#pragma ACCEL PIPELINE auto{__PIPE__L0}\n",
            "  \n",
            "#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n",
            "  \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n",
            "  for (t = 0; t < 40; t++) {\n",
            "    \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_0}\n",
            "    for (j = 0; j < 80; j++) {\n",
            "      ey[0][j] = _fict_[t];\n",
            "    }\n",
            "    \n",
            "#pragma ACCEL PIPELINE auto{__PIPE__L0_1}\n",
            "    \n",
            "#pragma ACCEL TILE FACTOR=auto{__TILE__L0_1}\n",
            "    \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_1}\n",
            "    for (i = 1; i < 60; i++) {\n",
            "      \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_1_0}\n",
            "      for (j = 0; j < 80; j++) {\n",
            "        ey[i][j] = ey[i][j] - 0.5 * (hz[i][j] - hz[i - 1][j]);\n",
            "      }\n",
            "    }\n",
            "    \n",
            "#pragma ACCEL PIPELINE auto{__PIPE__L0_2}\n",
            "    \n",
            "#pragma ACCEL TILE FACTOR=auto{__TILE__L0_2}\n",
            "    \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_2}\n",
            "    for (i = 0; i < 60; i++) {\n",
            "      \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_2_0}\n",
            "      for (j = 1; j < 80; j++) {\n",
            "        ex[i][j] = ex[i][j] - 0.5 * (hz[i][j] - hz[i][j - 1]);\n",
            "      }\n",
            "    }\n",
            "    \n",
            "#pragma ACCEL PIPELINE auto{__PIPE__L0_3}\n",
            "    \n",
            "#pragma ACCEL TILE FACTOR=auto{__TILE__L0_3}\n",
            "    \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_3}\n",
            "    for (i = 0; i < 59; i++) {\n",
            "      \n",
            "#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_3_0}\n",
            "      for (j = 0; j < 79; j++) {\n",
            "        hz[i][j] = hz[i][j] - 0.7 * (ex[i][j + 1] - ex[i][j] + ey[i + 1][j] - ey[i][j]);\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "//#pragma endscop\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "Act as an expert in High Level Synthesis, insert High Level Synthesis pragma to the folloing program. Reason your choice of High Level Synthesis pragma in comment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_l8m65Gubk"
      },
      "outputs": [],
      "source": [
        "programs = [\n",
        "        'processed-mm-krnl.cpp',\n",
        "        'processed-dotprod-krnl2.cpp',\n",
        "        'processed-dotprod-krnl.cpp',\n",
        "        'processed-ewmm-krnl.cpp',\n",
        "        'processed-jacobi_1d-krnl.cpp',\n",
        "        'processed-madd-krnl.cpp',\n",
        "        'processed-syrk-krnl.cpp',\n",
        "        'processed-trmm-krnl.cpp',\n",
        "        'processed-trmm-opt-krnl.cpp',\n",
        "        'processed-vadd-krnl.cpp'\n",
        "]\n",
        "context = {}\n",
        "for program in programs:\n",
        "  with open(program, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    context[program] = prompt+'Input:'+'\\n'.join(lines) + \"\\nOutput:\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLruvfORIf-j"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyyMFNTwBE4X"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aelkVXPNAqW",
        "outputId": "d295d580-bcf0-49af-9e97-675a770a8091"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 10%|█         | 1/10 [00:10<01:35, 10.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 20%|██        | 2/10 [00:20<01:22, 10.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 30%|███       | 3/10 [00:25<00:55,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 40%|████      | 4/10 [00:31<00:42,  7.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|█████     | 5/10 [00:39<00:37,  7.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 60%|██████    | 6/10 [00:45<00:27,  6.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 70%|███████   | 7/10 [00:59<00:27,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 80%|████████  | 8/10 [01:07<00:17,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 90%|█████████ | 9/10 [01:15<00:08,  8.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 10/10 [01:20<00:00,  8.03s/it]\n"
          ]
        }
      ],
      "source": [
        "lens = [2000]\n",
        "for len in lens:\n",
        "    for program in tqdm(context):\n",
        "        text = context[program]\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "        input_ids=inputs[\"input_ids\"].to(device)\n",
        "        completion = model.generate(input_ids=input_ids, max_length=len, attention_mask=attention_mask)\n",
        "        output = tokenizer.decode(completion[0])\n",
        "        output_idx = output.rfind(\"Output:\\n\")\n",
        "        with open('./outputs/fewshots/' + program[program.find('processed'):program.find('.cpp')]+str(len)+'.out','w') as f:\n",
        "            f.write(output[output_idx:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oii4w982iny6",
        "outputId": "87f9917e-966d-4342-9c38-046c36b00961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/data/ (stored 0%)\n",
            "  adding: content/data/processed-mm-krnl.cpp (deflated 44%)\n",
            "  adding: content/data/trmm-krnl.cpp (deflated 73%)\n",
            "  adding: content/data/processed-atax-medium_kernel.c (deflated 54%)\n",
            "  adding: content/data/outputs/ (stored 0%)\n",
            "  adding: content/data/outputs/fewshots/ (stored 0%)\n",
            "  adding: content/data/outputs/fewshots/processed-vadd-krnl2000.out (deflated 23%)\n",
            "  adding: content/data/outputs/fewshots/processed-trmm-krnl2000.out (deflated 43%)\n",
            "  adding: content/data/outputs/fewshots/processed-syrk-krnl2000.out (deflated 52%)\n",
            "  adding: content/data/outputs/fewshots/processed-dotprod-krnl2000.out (deflated 26%)\n",
            "  adding: content/data/outputs/fewshots/processed-ewmm-krnl2000.out (deflated 34%)\n",
            "  adding: content/data/outputs/fewshots/processed-trmm-opt-krnl2000.out (deflated 51%)\n",
            "  adding: content/data/outputs/fewshots/processed-mm-krnl2000.out (deflated 42%)\n",
            "  adding: content/data/outputs/fewshots/processed-madd-krnl2000.out (deflated 34%)\n",
            "  adding: content/data/outputs/fewshots/processed-dotprod-krnl22000.out (deflated 49%)\n",
            "  adding: content/data/outputs/fewshots/processed-jacobi_1d-krnl2000.out (deflated 45%)\n",
            "  adding: content/data/outputs/ape/ (stored 0%)\n",
            "  adding: content/data/outputs/ape/processed-vadd-krnl2000.out (stored 0%)\n",
            "  adding: content/data/outputs/ape/processed-trmm-krnl2000.out (deflated 43%)\n",
            "  adding: content/data/outputs/ape/processed-syrk-krnl2000.out (deflated 80%)\n",
            "  adding: content/data/outputs/ape/processed-dotprod-krnl2000.out (deflated 26%)\n",
            "  adding: content/data/outputs/ape/processed-ewmm-krnl2000.out (deflated 34%)\n",
            "  adding: content/data/outputs/ape/processed-trmm-opt-krnl2000.out (deflated 51%)\n",
            "  adding: content/data/outputs/ape/processed-mm-krnl2000.out (deflated 42%)\n",
            "  adding: content/data/outputs/ape/processed-madd-krnl2000.out (deflated 34%)\n",
            "  adding: content/data/outputs/ape/processed-dotprod-krnl22000.out (deflated 49%)\n",
            "  adding: content/data/outputs/ape/processed-jacobi_1d-krnl2000.out (deflated 48%)\n",
            "  adding: content/data/outputs/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/madd-krnl.cpp (deflated 37%)\n",
            "  adding: content/data/processed-3mm_kernel.c (deflated 69%)\n",
            "  adding: content/data/processed-dotprod-krnl2.cpp (deflated 50%)\n",
            "  adding: content/data/processed-vadd-krnl.cpp (deflated 23%)\n",
            "  adding: content/data/processed-syrk-krnl.cpp (deflated 49%)\n",
            "  adding: content/data/ewmm-krnl.cpp (deflated 37%)\n",
            "  adding: content/data/processed-madd-krnl.cpp (deflated 36%)\n",
            "  adding: content/data/processed-jacobi_1d-krnl.cpp (deflated 49%)\n",
            "  adding: content/data/jacobi_1d-krnl.cpp (deflated 57%)\n",
            "  adding: content/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/syrk-krnl.cpp (deflated 61%)\n",
            "  adding: content/data/automatic_prompt_engineer/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/packed-refs (deflated 33%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/refs/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/refs/heads/main (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/description (deflated 14%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/branches/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/config (deflated 35%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/info/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/info/exclude (deflated 28%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/index (deflated 64%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/HEAD (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/refs/heads/main (deflated 30%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/refs/remotes/origin/HEAD (deflated 30%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/logs/HEAD (deflated 30%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/objects/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/objects/pack/pack-95c90b7d7448758e30e0daeee30224841ecf79d9.idx (deflated 9%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/objects/pack/pack-95c90b7d7448758e30e0daeee30224841ecf79d9.pack (deflated 1%)\n",
            "  adding: content/data/automatic_prompt_engineer/.git/objects/info/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/configs/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/configs/instruction_induction.yaml (deflated 69%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/configs/truthful_qa.yaml (deflated 70%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/__init__.py (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/rhymes.json (deflated 94%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/common_concept.json (deflated 79%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/sentence_similarity.json (deflated 81%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/sentiment.json (deflated 77%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/num_to_verbal.json (deflated 91%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/orthography_starts_with.json (deflated 85%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/sum.json (deflated 89%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/translation_en-de.json (deflated 83%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/second_word_letter.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/first_word_letter.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/informal_to_formal.json (deflated 65%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/translation_en-es.json (deflated 85%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/active_to_passive.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/larger_animal.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/translation_en-fr.json (deflated 85%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/antonyms.json (deflated 84%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/taxonomy_animal.json (deflated 89%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/word_in_context.json (deflated 84%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/negation.json (deflated 73%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/singular_to_plural.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/synonyms.json (deflated 83%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/cause_and_effect.json (deflated 68%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/diff.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/induce/letters_list.json (deflated 89%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/rhymes.json (deflated 93%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/common_concept.json (deflated 79%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/sentence_similarity.json (deflated 77%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/sentiment.json (deflated 72%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/num_to_verbal.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/orthography_starts_with.json (deflated 85%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/sum.json (deflated 85%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/translation_en-de.json (deflated 82%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/second_word_letter.json (deflated 84%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/first_word_letter.json (deflated 84%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/informal_to_formal.json (deflated 65%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/translation_en-es.json (deflated 83%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/active_to_passive.json (deflated 89%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/larger_animal.json (deflated 88%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/translation_en-fr.json (deflated 83%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/antonyms.json (deflated 81%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/taxonomy_animal.json (deflated 91%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/word_in_context.json (deflated 82%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/negation.json (deflated 70%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/singular_to_plural.json (deflated 84%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/synonyms.json (deflated 81%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/cause_and_effect.json (deflated 69%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/diff.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/raw/execute/letters_list.json (deflated 87%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/rhymes.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/common_concept.json (deflated 97%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/sentence_similarity.json (deflated 88%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/sentiment.json (deflated 87%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/num_to_verbal.json (deflated 93%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/orthography_starts_with.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/sum.json (deflated 93%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/translation_en-de.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/second_word_letter.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/first_word_letter.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/informal_to_formal.json (deflated 97%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/translation_en-es.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/active_to_passive.json (deflated 95%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/larger_animal.json (deflated 93%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/translation_en-fr.json (deflated 91%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/antonyms.json (deflated 91%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/taxonomy_animal.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/word_in_context.json (deflated 87%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/negation.json (deflated 89%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/singular_to_plural.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/synonyms.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/cause_and_effect.json (deflated 97%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/diff.json (deflated 93%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/induction_input/letters_list.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/rhymes.json (deflated 67%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/common_concept.json (deflated 60%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/sentence_similarity.json (deflated 63%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/sentiment.json (deflated 66%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/num_to_verbal.json (deflated 61%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/orthography_starts_with.json (deflated 70%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/sum.json (deflated 56%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/translation_en-de.json (deflated 58%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/second_word_letter.json (deflated 59%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/first_word_letter.json (deflated 68%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/informal_to_formal.json (deflated 55%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/translation_en-es.json (deflated 50%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/active_to_passive.json (deflated 62%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/larger_animal.json (deflated 61%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/translation_en-fr.json (deflated 51%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/antonyms.json (deflated 63%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/taxonomy_animal.json (deflated 64%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/word_in_context.json (deflated 74%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/negation.json (deflated 53%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/singular_to_plural.json (deflated 60%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/synonyms.json (deflated 57%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/cause_and_effect.json (deflated 63%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/diff.json (deflated 71%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/annotations/letters_list.json (deflated 63%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/load_data.py (deflated 62%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/instruction_induction/README.md (deflated 54%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/TruthfulQA/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/TruthfulQA/TruthfulQA.csv (deflated 78%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/TruthfulQA/TruthfulQA_test.csv (deflated 77%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/TruthfulQA/TruthfulQA_train.csv (deflated 75%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/TruthfulQA/load_data.py (deflated 51%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/data/__init__.py (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/run_instruction_induction.py (deflated 71%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/run_truthful_qa.py (deflated 65%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/instruction_induction/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/instruction_induction/utility.py (deflated 75%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/instruction_induction/__init__.py (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/instruction_induction/exec_accuracy.py (deflated 70%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/TruthfulQA/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/TruthfulQA/fine_tuned_gpt_eval.py (deflated 70%)\n",
            "  adding: content/data/automatic_prompt_engineer/experiments/evaluation/__init__.py (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/configs/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/configs/default.yaml (deflated 71%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/configs/bandits.yaml (deflated 68%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/config.py (deflated 66%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/generate.py (deflated 69%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/generate.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/__init__.cpython-310.pyc (deflated 34%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/evaluate.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/data.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/llm.cpython-310.pyc (deflated 57%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/ape.cpython-310.pyc (deflated 58%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/config.cpython-310.pyc (deflated 42%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__pycache__/template.cpython-310.pyc (deflated 61%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/template.py (deflated 74%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/__init__.py (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/llm.py (deflated 80%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/ape.py (deflated 81%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluate.py (deflated 69%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/likelihood.py (deflated 73%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/__pycache__/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/__pycache__/__init__.cpython-310.pyc (deflated 33%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/__pycache__/likelihood.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/__pycache__/bandits.cpython-310.pyc (deflated 57%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/__init__.py (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/evaluation/bandits.py (deflated 72%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer/data.py (deflated 71%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/test_evaluate.py (deflated 72%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/test_bandits.py (deflated 60%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/test_llm.py (deflated 69%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/test_template.py (deflated 76%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/__init__.py (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/test_config.py (deflated 59%)\n",
            "  adding: content/data/automatic_prompt_engineer/tests/test_generate.py (deflated 57%)\n",
            "  adding: content/data/automatic_prompt_engineer/LICENSE.md (deflated 41%)\n",
            "  adding: content/data/automatic_prompt_engineer/1qkqywmMptdWtAEfFvauDSVhjJwP0ePJI (deflated 69%)\n",
            "  adding: content/data/automatic_prompt_engineer/.gitignore (deflated 52%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/navigate/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/navigate/task.json (deflated 96%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/gender_inclusive_sentences_german/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/gender_inclusive_sentences_german/task.json (deflated 81%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/tense/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/tense/task.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/movie_recommendation/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/movie_recommendation/task.json (deflated 82%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/winowhy/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/winowhy/task.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/ruin_names/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/ruin_names/task.json (deflated 88%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/timedial/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/timedial/task.json (deflated 80%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/logical_fallacy_detection/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/logical_fallacy_detection/task.json (deflated 89%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/operators/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/operators/task.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/question_selection/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/question_selection/task.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/snarks/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/snarks/task.json (deflated 80%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/task.json (deflated 86%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/implicatures/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/implicatures/task.json (deflated 83%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/word_unscrambling/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/word_unscrambling/task.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/linguistics_puzzles/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/linguistics_puzzles/task.json (deflated 83%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/causal_judgment/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/causal_judgment/task.json (deflated 88%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/object_counting/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/object_counting/task.json (deflated 90%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/dyck_languages/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/dyck_languages/task.json (deflated 98%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/hyperbaton/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/hyperbaton/task.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/presuppositions_as_nli/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/presuppositions_as_nli/task.json (deflated 93%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/word_sorting/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/word_sorting/task.json (deflated 71%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/disambiguation_qa/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/disambiguation_qa/task.json (deflated 92%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/sports_understanding/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/sports_understanding/generate_tasks.py (deflated 64%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/sports_understanding/vocab.json (deflated 71%)\n",
            "  adding: content/data/automatic_prompt_engineer/data/bigbench-ii/sports_understanding/task.json (deflated 93%)\n",
            "  adding: content/data/automatic_prompt_engineer/demo.py (deflated 83%)\n",
            "  adding: content/data/automatic_prompt_engineer/demo.ipynb (deflated 70%)\n",
            "  adding: content/data/automatic_prompt_engineer/README.md (deflated 67%)\n",
            "  adding: content/data/automatic_prompt_engineer/setup.py (deflated 51%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer.egg-info/ (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer.egg-info/SOURCES.txt (deflated 75%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer.egg-info/requires.txt (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer.egg-info/PKG-INFO (deflated 17%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer.egg-info/top_level.txt (stored 0%)\n",
            "  adding: content/data/automatic_prompt_engineer/automatic_prompt_engineer.egg-info/dependency_links.txt (stored 0%)\n",
            "  adding: content/data/processed-ewmm-krnl.cpp (deflated 35%)\n",
            "  adding: content/data/processed-atax_kernel.c (deflated 54%)\n",
            "  adding: content/data/processed-adi_kernel.c (deflated 69%)\n",
            "  adding: content/data/processed-dotprod-krnl.cpp (deflated 29%)\n",
            "  adding: content/data/trmm-opt-krnl.cpp (deflated 50%)\n",
            "  adding: content/data/vadd-krnl.cpp (deflated 26%)\n",
            "  adding: content/data/processed-doitgen_kernel.c (deflated 56%)\n",
            "  adding: content/data/processed-trmm-opt-krnl.cpp (deflated 52%)\n",
            "  adding: content/data/mm-krnl.cpp (deflated 44%)\n",
            "  adding: content/data/fewshots_examples/ (stored 0%)\n",
            "  adding: content/data/fewshots_examples/gesummv-medium_kernel.c (deflated 55%)\n",
            "  adding: content/data/fewshots_examples/bicg-medium_kernel.c (deflated 55%)\n",
            "  adding: content/data/fewshots_examples/heat-3d_kernel.c (deflated 74%)\n",
            "  adding: content/data/fewshots_examples/gemver_kernel.c (deflated 73%)\n",
            "  adding: content/data/fewshots_examples/gemm-ncubed_kernel.c (deflated 61%)\n",
            "  adding: content/data/fewshots_examples/fdtd-2d-large_kernel.c (deflated 75%)\n",
            "  adding: content/data/fewshots_examples/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/fewshots_examples/correlation_kernel.c (deflated 69%)\n",
            "  adding: content/data/fewshots_examples/fdtd-2d_kernel.c (deflated 75%)\n",
            "  adding: content/data/fewshots_examples/doitgen-red_kernel.c (deflated 61%)\n",
            "  adding: content/data/fewshots_examples/doitgen_kernel.c (deflated 61%)\n",
            "  adding: content/data/fewshots_examples/aes_kernel.c (deflated 82%)\n",
            "  adding: content/data/fewshots_examples/adi_kernel.c (deflated 72%)\n",
            "  adding: content/data/fewshots_examples/gemver-medium_kernel.c (deflated 73%)\n",
            "  adding: content/data/fewshots_examples/3mm_kernel.c (deflated 77%)\n",
            "  adding: content/data/fewshots_examples/gesummv_kernel.c (deflated 56%)\n",
            "  adding: content/data/fewshots_examples/bicg-large_kernel.c (deflated 52%)\n",
            "  adding: content/data/fewshots_examples/atax_kernel.c (deflated 58%)\n",
            "  adding: content/data/fewshots_examples/atax-medium_kernel.c (deflated 58%)\n",
            "  adding: content/data/processed-trmm-krnl.cpp (deflated 66%)\n",
            "  adding: content/data/dotprod-krnl2.cpp (deflated 52%)\n",
            "  adding: content/data/dotprod-krnl.cpp (deflated 30%)\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!zip -r /content/data2.zip /content/data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SER7g2AoiF0h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdDbHpqRPBMx"
      },
      "source": [
        "# Soft Prompting (APE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIE7MiiXPGKD"
      },
      "outputs": [],
      "source": [
        "input_programs = ['bicg-large_kernel.c', 'atax_kernel.c','atax-medium_kernel.c', '3mm_kernel.c', 'adi_kernel.c', 'doitgen_kernel.c', 'gemver_kernel.c',\n",
        "                  'bicg-medium_kernel.c','correlation_kernel.c', 'fdtd-2d-large_kernel.c', 'fdtd-2d_kernel.c']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB55QXeyv9Nm"
      },
      "outputs": [],
      "source": [
        "input = []\n",
        "output = []\n",
        "for i in range(len(input_programs)):\n",
        "  program_name = input_programs[i]\n",
        "  with open(f'fewshots_examples/{program_name}', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  input.append(''.join(lines))\n",
        "  lines = [line for line in lines if '#pragma' not in line]\n",
        "  output.append(''.join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "eoEv4yRExHdC",
        "outputId": "52ee0c92-48cb-46df-90fd-db1c14813e25"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#pragma ACCEL kernel\\n\\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\\n{\\n  int i;\\n  int j;\\n  int k;\\n//#pragma scop\\n/* E := A*B */\\n  \\n#pragma ACCEL PIPELINE auto{__PIPE__L0}\\n  \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\\n  \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\\n  for (i = 0; i < 40; i++) {\\n    \\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\\n    \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\\n    \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\\n    for (j = 0; j < 50; j++) {\\n      E[i][j] = 0.0;\\n      \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\\n      for (k = 0; k < 60; ++k) {\\n        E[i][j] += A[i][k] * B[k][j];\\n      }\\n    }\\n  }\\n/* F := C*D */\\n  \\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\\n  \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\\n  \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\\n  for (i = 0; i < 50; i++) {\\n    \\n#pragma ACCEL PIPELINE auto{__PIPE__L4}\\n    \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L4}\\n    \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\\n    for (j = 0; j < 70; j++) {\\n      F[i][j] = 0.0;\\n      \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7}\\n      for (k = 0; k < 80; ++k) {\\n        F[i][j] += C[i][k] * D[k][j];\\n      }\\n    }\\n  }\\n/* G := E*F */\\n  \\n#pragma ACCEL PIPELINE auto{__PIPE__L2}\\n  \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\\n  \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\\n  for (i = 0; i < 40; i++) {\\n    \\n#pragma ACCEL PIPELINE auto{__PIPE__L5}\\n    \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L5}\\n    \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\\n    for (j = 0; j < 70; j++) {\\n      G[i][j] = 0.0;\\n      \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L8}\\n      for (k = 0; k < 50; ++k) {\\n        G[i][j] += E[i][k] * F[k][j];\\n      }\\n    }\\n  }\\n//#pragma endscop\\n}\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "zVMxtWuysHzu",
        "outputId": "e359ca91-34f6-4aa3-d93e-192edbb2c0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.14.1\n",
            "    Uninstalling openai-1.14.1:\n",
            "      Successfully uninstalled openai-1.14.1\n",
            "Successfully installed openai-0.28.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b5d7a92e8a9346618243025f3ea25396",
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibi52lSpqyQC"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsC6yyTvqeI4"
      },
      "outputs": [],
      "source": [
        "eval_template = \\\n",
        "\"\"\"Instruction: [PROMPT]\n",
        "Input: [INPUT]\n",
        "Output: [OUTPUT]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3sckoR_5OQP"
      },
      "outputs": [],
      "source": [
        "prompt_gen_template = \"I gave an instruction to a High Level Synthesis expert. Based on the instruction they produced the following input-output pairs:\\n\\n[full_DEMO]\\n\\nThe instruction was to [APE]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lNobmPdqu7r",
        "outputId": "98840440-7dcc-40fe-98c2-dc722ffd4357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating prompts...\n",
            "[GPT_forward] Generating 10 completions, split into 1 batches of size 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model returned 10 prompts. Deduplicating...\n",
            "Deduplicated to 10 prompts.\n",
            "Evaluating prompts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating prompts:  90%|█████████ | 18/20 [00:30<00:03,  1.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rate limit reached for organization org-q5MbHIrUtti7qAqtgnkbZCH5 on tokens per min (TPM): Limit 250000, Used 114467, Requested 153360. Please try again in 4.278s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "Retrying...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating prompts: 100%|██████████| 20/20 [00:39<00:00,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished evaluating.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from automatic_prompt_engineer import ape\n",
        "\n",
        "result, demo_fn = ape.simple_ape(\n",
        "    dataset=(input, output),\n",
        "    eval_template=eval_template,\n",
        "    eval_model='davinci-002',\n",
        "    prompt_gen_model='davinci-002',\n",
        "    num_prompts=15,\n",
        "    eval_batch_size=500,\n",
        "    eval_rounds=20,\n",
        "    prompt_gen_template = prompt_gen_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtRPpzrsq3le",
        "outputId": "7cacee33-571f-4139-9f5b-e29994bfa49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " produce accelerated kernels. The accelerator instruction causes the kernel to be run using special acceleration techniques and to produce the parallel reduction information. The information is stored in a reduction list. The input-output pairs are generated by the compiler's automatic parallelisation and scheduling.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(result.prompts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPV1r4gYGjhS"
      },
      "outputs": [],
      "source": [
        "prompt = fewshots_header + \"/n\" + result.prompts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5qADLbMG5s7"
      },
      "outputs": [],
      "source": [
        "programs = [\n",
        "        'processed-mm-krnl.cpp',\n",
        "        'processed-dotprod-krnl2.cpp',\n",
        "        'processed-dotprod-krnl.cpp',\n",
        "        'processed-ewmm-krnl.cpp',\n",
        "        'processed-jacobi_1d-krnl.cpp',\n",
        "        'processed-madd-krnl.cpp',\n",
        "        'processed-syrk-krnl.cpp',\n",
        "        'processed-trmm-krnl.cpp',\n",
        "        'processed-trmm-opt-krnl.cpp',\n",
        "        'processed-vadd-krnl.cpp'\n",
        "]\n",
        "context = {}\n",
        "for program in programs:\n",
        "  with open(program, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    context[program] = prompt+'Input:'+'\\n'.join(lines) + \"\\nOutput:\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUdlkP9AHKUp",
        "outputId": "e18c58e0-19ef-4407-8fef-2de80d663f47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 10%|█         | 1/10 [00:09<01:27,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 20%|██        | 2/10 [00:20<01:21, 10.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 30%|███       | 3/10 [00:25<00:54,  7.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 40%|████      | 4/10 [00:31<00:42,  7.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|█████     | 5/10 [00:40<00:38,  7.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 60%|██████    | 6/10 [00:46<00:28,  7.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 70%|███████   | 7/10 [01:08<00:36, 12.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 80%|████████  | 8/10 [01:17<00:22, 11.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 90%|█████████ | 9/10 [01:25<00:10, 10.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 10/10 [01:58<00:00, 11.85s/it]\n"
          ]
        }
      ],
      "source": [
        "lens = [2000]\n",
        "for len in lens:\n",
        "    for program in tqdm(context):\n",
        "        text = context[program]\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "        input_ids=inputs[\"input_ids\"].to(device)\n",
        "        completion = model.generate(input_ids=input_ids, max_length=len, attention_mask=attention_mask)\n",
        "        output = tokenizer.decode(completion[0])\n",
        "        output_idx = output.rfind(\"Output:\\n\")\n",
        "        with open('./outputs/ape/' + program[program.find('processed'):program.find('.cpp')]+str(len)+'.out','w') as f:\n",
        "            f.write(output[output_idx:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayEXq5ryPUUI"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxcc56dDSzYp",
        "outputId": "57fc9a1d-06f5-4303-f1c1-76acbb34d472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.34.64-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.64 (from boto3)\n",
            "  Downloading botocore-1.34.64-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.64->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.64->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.64->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.64 botocore-1.34.64 jmespath-1.0.1 s3transfer-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bFFqMT3SPV8L"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "BUCKET_NAME = 'cs259project'\n",
        "s3 = boto3.resource('s3', aws_access_key_id = '',\n",
        "                          aws_secret_access_key= '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CK_xl32q8QpI"
      },
      "outputs": [],
      "source": [
        "s3.Bucket(BUCKET_NAME).download_file('rkirby-nemo-aligner.hls-rm-001.tar.gz', 'rkirby-nemo-aligner.hls-rm-001.tar.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPUdQ6p4_jLq",
        "outputId": "a85e42ae-bea1-4d66-b030-6cb0f5909690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting udocker\n",
            "  Downloading udocker-1.3.13-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/118.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m112.6/118.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: udocker\n",
            "Successfully installed udocker-1.3.13\n",
            "Info: creating repo: /root/.udocker\n",
            "Info: udocker command line interface 1.3.13\n",
            "Info: searching for udockertools >= 1.2.11\n",
            "Info: installing udockertools 1.2.11\n",
            "Info: installation of udockertools successful\n"
          ]
        }
      ],
      "source": [
        "!pip install udocker\n",
        "!udocker --allow-root install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FjdfmYtA3X8",
        "outputId": "bb3cd161-1683-4c46-aa86-0da4fa3a4a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Info: keyboard interrupt\n"
          ]
        }
      ],
      "source": [
        "!udocker --allow-root run hello-world\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69JyICyt_zYV",
        "outputId": "259de396-14bf-4bb7-e4df-a8d25e1ee876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Info: adding layer: d7f57ad09d27fe4b60916f041ad1a3771b71a09168ad930aaf9f0432dcdfa546\n",
            "Info: adding layer: aab3aa7fa37bf9f7434d7c491b34ec18920b6ea294d9ce9640f40a3162daa21d\n",
            "Info: adding layer: 522d9e0aa06cfdc0f8a246159be46d0cee3174a33a145c9696084dd3b8c28cec\n",
            "Info: adding layer: 82b7e323093fdf3ec4a1395cbac330e94fe39386a1d86d642f7e42e95fb31121\n",
            "Info: adding layer: c7b4a070076adcd37051dd6255c5191f14c1b53c136db71c7b7e833949198aa1\n",
            "Info: adding layer: d2b6758f8cae5523342b1bcc2452f7b60f69931641fede58f62e077edbd8fee9\n",
            "Info: adding layer: 73696fdda370e396ce3def1af7b5966a2e13cd02486dc7b7b4ecdbbba7643486\n",
            "Info: adding layer: 8348759e8f23ca47dcdc065b5e535778c9a3a8230e0d8ce9499d8b5109ca3c8c\n",
            "Info: adding layer: 5f130bb0a04adb1bee458e3b52bde8de2512280044e9b473436e8a090691ca20\n",
            "Info: adding layer: 9aecbd3dece1ae0428ea99796166df51fcb27a2d979c78f473441398360c653b\n",
            "Info: adding layer: 557572fd8f5ec2b6dcf18e726ba410bbfe67b4f1728846f58abc73a3148fcc28\n",
            "Info: adding layer: 9960c45766b6d978f94f410cc770a9421f5bb9c2aa0a9e39550020d8044f62c7\n",
            "Info: adding layer: 53aebfd59509fd6df52da5f0d28c72e70c34585bc7b939a77351bfab467b854b\n",
            "Info: adding layer: de9620543111b707557b3f19d819db3ace17deeab767ac6c08340cfe06f98943\n",
            "Info: adding layer: b264f233b1eddde654ded8da2570b3ac09e3eb9981711afde0d9118468facadd\n",
            "Info: adding layer: 0ec86fdb4eaecd0beadf9b3b015414f715cfcc9da35c961aa394a474b410885d\n",
            "Info: adding layer: 00c5865c450de31ab02d0775ed53236a87e706104ff74ab133d96f2d62c8c852\n",
            "Info: adding layer: b80d1f0d7b6f6827117627f6db135a5c29f0b787efcf6b1476a6de5044edaa4b\n",
            "Info: adding layer: 6c1ec4dd27d397cadf6fe30aed6e957beebbe90a3a2a6026716e716ed8b15dd5\n",
            "Info: adding layer: c80da1d96720da337398901d345cab66260257ac93f248ecee0c398c7836c81f\n",
            "Info: adding layer: 682ba7f6a3902af1e2d29acfcb082e6a158165b7e0e9fba612ebbcf5707ea32c\n",
            "Info: adding layer: ca16b98d8f6456afd66ae1d11ad3b99ceeab6b3ab476af7dd3cb8b9ec8767201\n",
            "Info: adding layer: 085d971219f00d33abf24b3fa49f92b8818bf5dc52e67fb09bc2eb029e247b7b\n",
            "Info: adding layer: 0ca5ea6ecff7eb27dec49cd81f405fbe2a575f53c7cf2ee8e8a0739642f2ecc9\n",
            "Info: adding layer: a08008fea23bbb5b8e6d07c4f52e24d92121b4f15b47498d8ffd687ef0149423\n",
            "Info: adding layer: 0bf859f6e4a2988bfd4ad186ec3f600922c2e7e55452a1307a7cc4cdf8395e50\n",
            "Info: adding layer: 92b3eaeb1401cbb11222d3105e660a0f3b69cea35620fc640f576328b0f773d0\n",
            "Info: adding layer: cd5fc0feb1e9333325f96bc5c71751ad86f2d4476f367d81327a9e455bc10fe4\n",
            "Info: adding layer: 71214aee12f3c3e75039b54de3ed5402c39676bf21265f55484ea6db1bed8312\n",
            "Info: adding layer: 60b6b0cd9c405b86d56f7c7f65974251d574f64e88f10f948c25558d1a8856ec\n",
            "Info: adding layer: 9daa0fab18a4c90beb56858148d6c828d5268c2929169fd2f47d750874ddc33f\n",
            "Info: adding layer: ce0da7daa95f2dcb8b57d96ca75849458804c2d5d452264eb54e11dc8096a23f\n",
            "Info: adding layer: 6da19e3c35a4f48f69caee6775bcbc562680e059b54477a594eed103afee4259\n",
            "Info: adding layer: ae18f0f48d85158b47560957f9eaba092022d3190c79a7f0c1dad971c319ae00\n",
            "Info: adding layer: 2f388c7d063fc3999bce67d2c049391648da2a4cc432483fd73c4ae0b154c043\n",
            "Info: adding layer: 26e7521b2f26332cfc9289d7c3fe0642915d128881ce2ef25213703ad6cfe2b4\n",
            "Info: adding layer: 228c718b8bbe8008b3f44def28c8d43982e3f6dc1590bf3c9e6769bf3112bebd\n",
            "Info: adding layer: 91c2fa9baf1396d4bdbf736ec14375ecf1e4aafe057e88c8f3e09922cf16a8f8\n",
            "Info: adding layer: 8ea49bbbc70bdb32c7fb03cd7d1abd21089aea2c81758015949aabd82cfa17d0\n",
            "Info: adding layer: 532c6916958d4810a9209804515d6125e83baedfaed8ea71e85454f17aa97138\n",
            "Info: adding layer: be9be63097d0d4db7849e1e4ee1145878671df02da9ce77ee3a468d75e565756\n",
            "Info: adding layer: 578cb4c9c7b5c39cfddb689497c895fc4b75d0a1bf74d055aa392d4ad79072ae\n",
            "Info: adding layer: 0218a023057f6c1d015dde51eb5855741b440395031dad072bc9ad878d07c017\n",
            "Info: adding layer: 407fc224dc44ca30c24473d70690f76a4c6696915a1fbf46a68d9edaa2caf125\n",
            "Info: adding layer: 6654784367a6c1c76d8e86896e625ab7fd43cf2d126f91d63f69d52a73d8f39f\n",
            "Info: adding layer: fdb64e8a65089a76e280fb17bb48748a0cd5dcdb5ab821d47053f2c24f74f719\n",
            "Info: adding layer: f3f9c708a0926bc3b4f4f3f9c50a9003b65720cca17af2fba95ab7a20586d9b7\n",
            "Info: adding layer: 35913af4f83df933e479e09e93056a0a0ebc4602f87cb1caea789202f7bba345\n",
            "Info: adding layer: c9c3bed2e9cd9e2bf5237c459a2c5d57ec6754b82800cda97bb762a431737203\n",
            "Info: adding layer: df18fcc56211c525233dce8eb713596791cd22516c3ca9b6a346052d7fa02cff\n",
            "Info: adding layer: a0010538ed64331f5fded6c4346668b2fe2a176ff86d1a312bb9014d0de01f07\n",
            "Info: adding layer: bf24621f874f63164236d731b124b880418a839d1611ce9580786b18c41a23e5\n",
            "Info: adding layer: f496e1925468d1ae937aa309e5d38a837865ba915456f5f8b7e7ffb7acd6e9bc\n",
            "Info: adding layer: 6e55fcc5029c92bffba914c42268d7eaf4adb0fd76b0e235d97176ca3c2491d7\n",
            "Info: adding layer: c8f14bb8477ae605af86cbb790c3f9e4354ccb19a4abd6858222265f27eb1876\n",
            "Info: adding layer: fb74490bdffad442cc7dc619943abd1b5682cc1b7f89cec109760434fb47613f\n",
            "Info: adding layer: 9a542c9e0570bb697faa2f220b4b69125a1f5bc71ef849222923a3e5f54d2cae\n",
            "Info: adding layer: 3ccb577105f69e42f7727edb3d7b56d08498bd5a471b27983d512836fdad7a62\n",
            "Info: adding layer: da609ff561be895141dbf519f6d9d207e3374e716a45e603f8b85a6a2f4f6b20\n",
            "Info: adding layer: 729bb7391cd05abd8446dfa196af067335991fa0c5545de1924ca244861985ab\n",
            "Info: adding layer: f3384784096565a93b2cd66b9c5694e93a989b3a90e9033c4078a35a1a7d5f40\n",
            "Info: adding layer: 5f7b8aa51788c1b0e95763e7919741ca6c905f430a3c7df24c400e5805ff8b69\n",
            "Info: adding layer: 8821f7289b73e0eb96e7989790cca826d4ebf6304b333a8a14ec832678ae3f8c\n",
            "Info: adding layer: 2604281df287a2da3e0560a3c9c61b9adb1819bd9a353620e8cfed3af6f1abce\n",
            "Info: adding layer: 1381611552cd314a405d7bcd5a73b9962e7d7ab4b9667c261b36825fe3ff4348\n",
            "Info: adding layer: a41d2e64ed7efd571a880793b11c4e3b84c556f86a9f72b4051b690815ad20f5\n",
            "nvcr.io/nvidian/rkirby-nemo-aligner:hls-rm\n"
          ]
        }
      ],
      "source": [
        "!udocker --allow-root load -i rkirby-nemo-aligner.hls-rm-001.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azJ7DBnh_2fb",
        "outputId": "e249b27c-123a-4202-e651-b16d413d390e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REPOSITORY\n",
            "nvcr.io/nvidian/rkirby-nemo-aligner:hls-rm    .\n"
          ]
        }
      ],
      "source": [
        "!udocker --allow-root images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFHCDXUXEdX7",
        "outputId": "6de1126d-a111-4326-a057-acdb2ee45eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: create container: getting layers or json\n",
            "Error: manifest not found or not authorized\n",
            "Error: no files downloaded\n",
            "Error: create container: getting layers or json\n",
            "Error: image or container not available\n"
          ]
        }
      ],
      "source": [
        "!udocker --allow-root run nvcr.io/nvidian/rkirby-nemo-aligner:hls-rm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBmFCrFiEjB3",
        "outputId": "652b6481-9bbe-45a2-ea3b-baf35e26c5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: invalid container id\n"
          ]
        }
      ],
      "source": [
        "!udocker --allow-root setup  --nvidia nvcr.io/nvidian/rkirby-nemo-aligner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1rJdbEgJi1m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
